{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=center>Capítulo 2 - Operações principais com spaCy</h1>\n",
    "<p align=center><img src=https://www.edivaldobrito.com.br/wp-content/uploads/2021/02/spacy-uma-biblioteca-de-processamento-de-linguagem-natural.jpg width=500></p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neste capítulo, você aprenderá as principais operações com **spaCy**, como criar um pipeline de linguagem, tokenizar o texto e dividir o texto em frases.\n",
    "\n",
    "Primeiro, você aprenderá o que é um pipeline de processamento de linguagem e os componentes do pipeline. Continuaremos com as convenções gerais de spaCy – aulas importantes e organização de classes – para ajudá-lo a entender melhor a organização da biblioteca spaCy e desenvolver uma compreensão sólida da própria biblioteca.\n",
    "\n",
    "Você aprenderá então sobre o primeiro componente do pipeline – **Tokenizer**. Você também aprenderá sobre um importante conceito linguístico – **lematização** – juntamente com suas aplicações na **compreensão da linguagem natural (NLU)**.\n",
    "\n",
    "Em seguida, abordaremos as **classes de contêiner** e as **estruturas de dados spaCy** em detalhes. Terminaremos o capítulo com recursos úteis que você usará no desenvolvimento diário de PNL.\n",
    "\n",
    "## Visão geral das convenções spaCy\n",
    "Cada aplicação de PNL consiste em várias etapas de processamento do texto. Como você pode ver no primeiro capítulo, sempre criamos instâncias chamadas **nlp** e **doc*. Mas o que fizemos exatamente?\n",
    "\n",
    "Quando chamamos **nlp** em nosso texto, **spaCy** aplica algumas etapas de processamento. A primeira etapa é a tokenização para produzir um objeto **Doc**. O objeto **Doc** é então processado com um **tagger**, um **analisador (parser)** e um **reconhecedor de entidade (entity recognizer)**. Essa maneira de processar o texto é chamada de **pipeline de processamento de linguagem**. Cada componente da pipeline retorna o **Doc** processado e o passa para o próximo componente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><img src=https://blog.neurotech.africa/content/images/2022/03/spaCy-nlp.png width=900></p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um objeto de **pipeline spaCy** é criado quando carregamos um modelo de linguagem. Carregamos um modelo em inglês e inicializamos um pipeline no seguinte segmento de código:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('I went there')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O que aconteceu exatamente no código anterior é o seguinte:\n",
    "1. Começamos importando **spaCy**.\n",
    "2. Na segunda linha, **spacy.load()** retornou uma instância da classe **Language**, **nlp**. A classe **Language** é o *pipeline de processamento de texto*.\n",
    "3. Depois disso, aplicamos **nlp** na frase de exemplo **\"I went there\"** e peguei uma instância da classe **Doc**, **doc**.\n",
    "\n",
    "A classe **Language** aplica todas as etapas anteriores do pipeline à sua frase de entrada nos bastidores. Depois de aplicar **nlp** à sentença, o objeto **Doc** contém tokens que são marcados, lematizados e marcados como entidades se o token for uma entidade (então entraremos em detalhes sobre o que são e como isso é feito posteriormente). Cada componente do pipeline tem uma tarefa bem definida:\n",
    "\n",
    "<img src=\"images/pipeline_components.PNG\" width=900>\n",
    "\n",
    "O pipeline de processamento de linguagem **spaCy** sempre depende do modelo estatístico e de seus recursos. É por isso que sempre carregamos um modelo de linguagem com **spacy.load()** como o primeiro passo em nosso código.\n",
    "\n",
    "Cada componente corresponde a uma classe **spaCy**. As classes **spaCy** têm nomes autoexplicativos, como **Language**, **Doc** e **Vocab**. Já usamos as classes **Language** e **Doc** – vamos ver todas as classes do pipeline de processamento e suas funções:\n",
    "\n",
    "<img src=\"images/processing_pipeline.png\">\n",
    "\n",
    "Não fique intimidade com o número de classes. Cada classe tem um característica única para nos ajudar a processar o texto melhor.\n",
    "\n",
    "Existem mais estruturas de dados para representar os dados de texto e os dados da linguagem. A classe Conteiner como a Doc armazena as informações sobre as sentenças, palavras e o texto. Existem outras classes conteiner além da Doc:\n",
    "\n",
    "<img src=\"images/conteiner_classes.PNG\" width=900>\n",
    "\n",
    "Finalmente, spaCy fornece classes auxiliares para vetores, vocabulário de linguagem e anotações. Veremos a classe **Vocab** frequentemente neste livro. **Vocab** representa o vocabulário de uma língua. Vocab contém todas as palavras do modelo de linguagem que carregamos:\n",
    "\n",
    "<img src=\"images/outras_classes.PNG\" width=900>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As estruturas de dados do backbone da biblioteca spaCy são **Doc** e **Vocab**. O objeto **Doc** abstrai o texto possuindo a sequência de tokens e todas as suas propriedades. O objeto **Vocab** fornece um conjunto centralizado de **strings** e atributos léxicos para todas as outras classes. Dessa forma, o **spaCy** evita o armazenamento de várias cópias de dados linguísticos:\n",
    "\n",
    "<img src=\"images/diagrama_spacy.PNG\">\n",
    "\n",
    "Você pode dividir os objetos que compõem a arquitetura spaCy anterior em dois: **contêineres** e **componentes de pipeline de processamento**. Neste capítulo, primeiro aprenderemos sobre dois componentes básicos, **Tokenizer** e **Lemmatizer**, depois exploraremos mais os objetos **Container**.\n",
    "\n",
    "**spaCy** faz todas essas operações para nós nos bastidores, permitindo que nos concentremos no desenvolvimento de nosso próprio aplicativo. Com esse nível de abstração, usar **spaCy** para desenvolvimento de aplicativos NLP não é coincidência. Vamos começar com a classe **Tokenizer** e ver o que ela oferece para nós; em seguida, exploraremos todas as classes de contêineres uma a uma ao longo do capítulo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apresentando a tokenização\n",
    "Vimos que a primeira etapa em um pipeline de processamento de texto é a tokenização. A tokenização é sempre a primeira operação porque todas as outras operações requerem os tokens.\n",
    "\n",
    "Tokenização significa simplesmente dividir a sentença em seus tokens. Um token é uma unidade de semântica. Você pode pensar em um token como a menor parte significativa de um pedaço de texto. Os tokens podem ser palavras, números, pontuação, símbolos de moeda e quaisquer outros símbolos significativos que são os blocos de construção de uma frase. Seguem exemplos de tokens:\n",
    "* USA\n",
    "* N.Y\n",
    "* 33\n",
    "* 3rd\n",
    "* !\n",
    "* ...\n",
    "* ?\n",
    "* 's\n",
    "\n",
    "A entrada para o tokenizer **spaCy** é um texto Unicode e o resultado é um objeto **Doc**. O código a seguir mostra o processo de tokenização:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'm', 'own', 'Giger', 'cat', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Im own Giger cat.\")\n",
    "print([token.text for token in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O seguinte é o que acabamos de fazer:\n",
    "1. Começamos importando **spaCy**.\n",
    "2. Em seguida, carregamos o modelo de idioma inglês por meio do atalho **en** para criar uma instância da **classe nlp Language**.\n",
    "3. Em seguida, aplicamos o objeto **nlp** à sentença de entrada para criar um objeto **Doc**, **doc**. Um objeto **Doc** é um contêiner para uma sequência de objetos **Token**. **spaCy** gera os objetos **Token** implicitamente quando criamos o objeto **Doc**.\n",
    "4. Finalmente, imprimimos uma lista dos tokens da sentença anterior.\n",
    "\n",
    "É isso, fizemos a tokenização com apenas três linhas de código. Você pode visualizar a tokenização com indexação da seguinte forma:\n",
    "\n",
    "<img src=\"images/tokenização_im_own_giver_cat.PNG\" width=500>\n",
    "\n",
    "Como os exemplos sugerem, a tokenização pode realmente ser complicada. Há muitos aspectos aos quais devemos prestar atenção: pontuação, espaços em branco, números e assim por diante. Dividir os espaços em branco com **text.split(\" \")** pode ser tentador e parece que está funcionando para a frase de exemplo que *I own a ginger cat*.\n",
    "\n",
    "Que tal a frase **\"It´s been a crazy week!!!\"**? Se fizermos um **split(\" \")** os tokens resultantes seriam **It's, been, a, crazy, week!!!**, que não é o que você deseja. Em primeiro lugar, **It's** não é um token, são dois tokens: **it** e **'s**. **week!!!** não é um token válido, pois a pontuação não está dividida corretamente. Além disso, **!!!** deve ser tokenizado por símbolo e deve gerar três **!**. Isso pode não parecer um detalhe importante, mas acredite, é importante para a análise de sentimentos. Vamos ver o que o tokenizer **spaCy** gerou:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"It's been a crazy week!!!\")\n",
    "print([token.text for token in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como o spaCy sabe onde dividir a frase? Ao contrário de outras partes do pipeline, o tokenizer não precisa de um modelo estatístico. A tokenização é baseada em regras específicas do idioma.\n",
    "\n",
    "As exceções do **tokenizer** definem regras para exceções, como **it's**, **don't**, **won't**, abreviaturas e assim por diante. você verá que as regras se parecem com {ORTH: \"n't\", LEMMA:\"not\"}, que descreve a regra de divisão de n't para o tokenizer.\n",
    "\n",
    "Os prefixos, sufixos e infixos descrevem principalmente como lidar com pontuação – por exemplo, dividimos em um ponto se estiver no final da frase, caso contrário, provavelmente é parte de uma abreviação como N.Y. e não deveríamos toque isso. Aqui, **ORTH** significa o texto e **LEMMA** significa a forma da palavra base sem quaisquer inflexões. O exemplo a seguir mostra a execução do algoritmo de tokenização spaCy:\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*Sibm12vBIZTjTDBp3I7yeQ.png width=500>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As regras de tokenização dependem das regras gramaticais do idioma individual. As regras de pontuação, como pontos de divisão, vírgulas ou pontos de exclamação, são mais ou menos semelhantes para muitos idiomas; no entanto, algumas regras são específicas para o idioma individual, como palavras de abreviação e uso de apóstrofos. **spaCy** suporta cada idioma com suas próprias regras específicas, permitindo dados e regras codificados à mão, pois cada idioma tem sua própria subclasse.\n",
    "\n",
    "> **Dica**\n",
    "\n",
    "> **spaCy** fornece tokenização *não destrutiva*, o que significa que sempre poderemos recuperar o texto original dos tokens. As informações de espaço em branco e pontuação são preservadas durante a tokenização, portanto, o texto de entrada é preservado como está.\n",
    "> Cada objeto **Language** contém um objeto **Tokenizer**. A classe **Tokenizer** é a classe que executa a tokenização. Você não costuma chamar essa classe diretamente quando cria uma instância de classe **Doc**, enquanto a classe **Tokenizer** atua nos bastidores. Quando queremos customizar a tokenização, precisamos interagir com essa classe. Vamos ver como é feito.\n",
    "\n",
    "## Personalizando o tokenizer\n",
    "Quando trabalhamos com um domínio específico, como medicina, seguros ou finanças, muitas vezes nos deparamos com palavras, abreviações e entidades que precisam de atenção especial. A maioria dos domínios que você processará tem palavras e frases características que precisam de regras de tokenização personalizadas. Veja como adicionar uma regra de caso especial a uma instância de classe **Tokenizer** existente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemme', 'that']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('lemme that')\n",
    "print([w.text for w in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "special_case = [{ORTH:'lem'},{ORTH:'me'}]\n",
    "nlp.tokenizer.add_special_case('lemme',special_case)\n",
    "print([w.text for w in nlp('lemme that')])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui está o que nós fizemos:\n",
    "1. Começamos novamente importando o **spaCy**.\n",
    "2. Em seguida, importamos o símbolo **ORTH**, que significa ortografia; isto é, texto.\n",
    "3. Continuamos com a criação de um objeto de classe **Language**, **nlp**, e criamos um objeto *Doc*, **doc**.\n",
    "4. Definimos um caso especial, onde a palavra **lemme** deve ser tokenizada como dois tokens, **lem** e **me**.\n",
    "5. Adicionamos a regra ao tokenizer do objeto **nlp**.\n",
    "6. A última linha mostra como a nova regra funciona.\n",
    "\n",
    "Quando definimos regras personalizadas, as regras de divisão de pontuação ainda serão aplicadas. Nosso caso especial será reconhecido como resultado, mesmo que esteja cercado por pontuação. O tokenizer dividirá a pontuação passo a passo e aplicará o mesmo processo à substring restante:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', '!']\n"
     ]
    }
   ],
   "source": [
    "print([w.text for w in nlp('lemme!')])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se você definir uma regra de caso especial com pontuação, a regra de caso especial terá precedência sobre a divisão de pontuação:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...lemme...?']\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"...lemme...?\", [{\"ORTH\": \"...lemme...?\"}])\n",
    "print([w.text for w in nlp(\"...lemme...?\")])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Dica profissional**\n",
    "Modifique o tokenizer adicionando novas regras *somente se você realmente precisar*. Confie em mim, você pode obter resultados bastante inesperados com regras personalizadas. Um dos casos em que você realmente precisa é ao trabalhar com o texto do *Twitter*, que geralmente está cheio de *hashtags* e símbolos especiais. Se você tiver texto de mídia social, primeiro insira algumas frases no *pipeline spaCy NLP* e veja como a tokenização funciona.\n",
    "\n",
    "## Depurando o tokenizer\n",
    "A biblioteca spaCy possui uma ferramenta para depuração:\n",
    "\n",
    "**nlp.tokenizer.explain(sentence)**. Ele retorna tuplas **(tokenizer rule/pattern,token)** para nos ajudar a entender o que aconteceu exatamente durante a tokenização. Vejamos um exemplo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let  --------------> SPECIAL-1\n",
      "'s  --------------> SPECIAL-2\n",
      "go  --------------> TOKEN\n",
      "!  --------------> SUFFIX\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Let's go!\"\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "for t in tok_exp:\n",
    "    print(f\"{t[1]}  --------------> {t[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No código anterior, importamos o **spaCy** e criamos uma instância da classe **Language**, **nlp**, como de costume. Em seguida, criamos uma instância da classe *Doc* com a frase **Let's go!**. Em seguida, solicitamos à instância da classe *Tokenizer*, **tokenizer**, do *nlp* uma explicação sobre a tokenização desta sentença. **nlp.tokenizer.explain()** explicou as regras que o tokenizer usou uma a uma.\n",
    "\n",
    "Depois de dividir uma frase em seus tokens, é hora de dividir um texto em suas frases.\n",
    "\n",
    "## Segmentação de frases\n",
    "Vimos que quebrar uma sentença em seus tokens não é uma tarefa simples. Que tal quebrar um texto em frases? É realmente um pouco mais complicado marcar onde uma frase começa e termina devido aos mesmos motivos de pontuação, abreviações e assim por diante.\n",
    "\n",
    "As sentenças de um objeto *Doc* estão disponíveis através da propriedade **doc.sents**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I flied to N.Y yesterday.\n",
      "It was around 5 pm.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determinar os limites da frase é uma tarefa mais complicada do que a tokenização. Como resultado, spaCy usa o analisador de dependência para realizar a segmentação de sentenças. Esta é uma característica única do spaCy – nenhuma outra biblioteca põe em prática uma ideia tão sofisticada. Os resultados são muito precisos em geral, a menos que você processe texto de um gênero muito específico, como do domínio da conversa ou texto de mídia social.\n",
    "\n",
    "Agora sabemos como segmentar um texto em frases e tokenizar as frases. Estamos prontos para processar os tokens um por um. Vamos começar com a *lematização*, uma operação comumente usada em semântica, incluindo análise de sentimentos.\n",
    "\n",
    "**Entendendo a lematização**\n",
    "\n",
    "Um **lemma** é a forma base de um token. Você pode pensar em um *lemma* como a forma na qual o token aparece em um dicionário. Por exemplo, o *lemma* de *eating* é *eat*; o *lemma* de *eats* é *eat*; *ate* da mesma forma mapeia para *eat*. *Lematização* é o processo de reduzir as formas das palavras aos seus *lemmas*. O código a seguir é um exemplo rápido de como fazer lematização com spaCy:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: I == LEMMA: I\n",
      "TOKEN: went == LEMMA: go\n",
      "TOKEN: there == LEMMA: there\n",
      "TOKEN: for == LEMMA: for\n",
      "TOKEN: working == LEMMA: work\n",
      "TOKEN: and == LEMMA: and\n",
      "TOKEN: worked == LEMMA: work\n",
      "TOKEN: for == LEMMA: for\n",
      "TOKEN: 3 == LEMMA: 3\n",
      "TOKEN: years == LEMMA: year\n",
      "TOKEN: . == LEMMA: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I went there for working and worked for 3 years.\")\n",
    "for token in doc:\n",
    "    print(f\"TOKEN: {token.text} == LEMMA: {token.lemma_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Até agora, você deve estar familiarizado com o que as três primeiras linhas do código fazem. Lembre-se de que importamos a biblioteca **spacy**, carregamos um modelo em inglês usando **spacy.load**, criamos um pipeline e aplicamos o pipeline à frase anterior para obter um objeto **Doc**. Aqui nós iteramos sobre tokens para obter seu *texto* e *lemmas*.\n",
    "\n",
    "Este é um lemma pronome, um símbolo especial para lemas de pronomes pessoais. Esta é uma exceção para fins semânticos: os pronomes pessoais *você*, *eu*, *tu*, *ele*, *dele* e assim por diante, parecem diferentes, mas em termos de significado, eles estão no mesmo grupo. *spaCy* oferece este truque para os lemas do pronome.\n",
    "\n",
    "Não se preocupe se tudo isso soar muito abstrato – vamos ver a lematização em ação com um exemplo do mundo real.\n",
    "\n",
    "### Lematização em NLU\n",
    "\n",
    "A lematização é um passo importante na NLU. Veremos um exemplo nesta subseção. Suponha que você crie um pipeline de NLP para um sistema de reserva de passagens. Seu aplicativo processa a sentença de um cliente, extrai dela as informações necessárias e a passa para a API de reservas.\n",
    "\n",
    "O pipeline de NLP deseja extrair a forma da viagem (*a flight*, *bus*, ou *train*), a *cidade de destino* e a *data*. A primeira coisa que o aplicativo precisa verificar é o meio de viagem:\n",
    "\n",
    "* *fly* – *flight* – *airway* – *airplane* - *plane*\n",
    "\n",
    "* *bus*\n",
    "\n",
    "* *railway* – *train*\n",
    "\n",
    "Temos essa lista de palavras-chave e queremos reconhecer os meios de viagem pesquisando os tokens na lista de palavras-chave. A maneira mais compacta de fazer essa pesquisa é pesquisar o lemma do token. Considere as seguintes frases de clientes:\n",
    "\n",
    "* List me all flights to Atlanta.\n",
    "* I need a flight to NY.\n",
    "* I flew to Atlanta yesterday evening and forgot my baggage.\n",
    "\n",
    "Aqui, não precisamos incluir todas as formas de palavras do verbo *fly* (*fly, flying, flies, flew, and flown*) na lista de palavras-chave e similares para a palavra **flight**; reduzimos todas as variantes possíveis para as formas básicas – *fly* e *flight*. Não pense apenas em inglês; línguas como espanhol, alemão e finlandês também têm muitas formas de palavras de um único lemma.\n",
    "\n",
    "A *lematização* também é útil quando queremos reconhecer a cidade de destino. Existem muitos apelidos disponíveis para cidades globais e a API de reservas pode processar apenas os nomes oficiais. O tokenizer e o lematizer padrão não saberão a diferença entre o nome oficial e o apelido. Nesse caso, você pode adicionar regras especiais, como vimos na seção *Introdução à tokenização*. O código a seguir desempenha um pequeno truque:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: I == LEMMA: i\n",
      "TOKEN: am == LEMMA: am\n",
      "TOKEN: flying == LEMMA: flying\n",
      "TOKEN: to == LEMMA: to\n",
      "TOKEN: Angeltown == LEMMA: Los Angeles\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH, NORM, LEMMA\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "special_case = [{ORTH: 'Angeltown', NORM: 'Los Angeles'}]\n",
    "nlp.tokenizer.add_special_case('Angeltown', special_case)\n",
    "doc = nlp(u'I am flying to Angeltown')\n",
    "for token in doc:\n",
    "    print(f\"TOKEN: {token.text} == LEMMA: {token.norm_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos um caso especial para a palavra Angeltown substituindo seu lema pelo nome oficial Los Angeles. Em seguida, adicionamos esse caso especial à instância do Tokenizer. Quando imprimimos os lemas token, vemos que Angeltown mapeia para Los Angeles como desejávamos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um *lemma* é a forma base de uma palavra e é sempre um membro do vocabulário da língua. O radical não precisa ser uma palavra válida. Por exemplo, o lemma da *improvement* é *improvement*, mas o radical é *improv*. Você pode pensar no radical como a menor parte da palavra que carrega o significado. Compare os seguintes exemplos:\n",
    "<table border='1'>\n",
    "<tr>\n",
    "<td>Palavra</td>\n",
    "<td>Lemma</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>university</td>\n",
    "<td>university</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universe</td>\n",
    "<td>universal</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universal</td>\n",
    "<td>universal</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universities</td>\n",
    "<td>university</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improvement</td>\n",
    "<td>improvement</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improvements</td>\n",
    "<td>improvements</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improves</td>\n",
    "<td>improve</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Os exemplos de *lemmas* de palavras anteriores mostram como o lemma é calculado seguindo as regras gramaticais do idioma. Aqui, o *lemma* de uma forma plural é a forma singular, e o lema de um verbo de terceira pessoa é a forma básica do verbo. Vamos compará-los com os seguintes exemplos de pares de radicais de palavras:\n",
    "<table border='1'>\n",
    "<tr>\n",
    "<td>Palavra</td>\n",
    "<td>Raíz</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>university</td>\n",
    "<td>univers</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universe</td>\n",
    "<td>univer</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universal</td>\n",
    "<td>univers</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>universities</td>\n",
    "<td>universi</td>\n",
    "<tr>\n",
    "<td>universes</td>\n",
    "<td>univers</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improvement</td>\n",
    "<td>improv</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improvements</td>\n",
    "<td>improv</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>improves</td>\n",
    "<td>improv</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "O primeiro e mais importante ponto a ser observado nos exemplos anteriores é que o *lemma* não precisa ser uma palavra válida na linguagem. O segundo ponto é que muitas palavras podem mapear para o mesmo radical. Além disso, palavras de diferentes categorias gramaticais podem ser mapeadas para o mesmo radical; aqui, por exemplo, o substantivo *improvement* e o verbo *improves* ambos mapeiam para *improv*.\n",
    "\n",
    "Embora os radicais não sejam palavras válidas, eles ainda carregam significado. É por isso que o *stemming* é comumente usado em aplicativos NLU.\n",
    "\n",
    "Algoritmos de *Stemming* não sabem nada sobre a gramática da língua. Essa classe de algoritmos funciona cortando alguns sufixos e prefixos comuns do início ou do final da palavra.\n",
    "\n",
    "Os algoritmos de *Stemming* são grosseiros, eles cortam a palavra da cabeça e da cauda. Existem vários algoritmos de stemming disponíveis para inglês, incluindo *Porter* e *Lancaster*. Você pode jogar com diferentes algoritmos de *stemming* na página de demonstração do *NLTK em https://text-processing.com/demo/stem/*.\n",
    "\n",
    "A lematização, por outro lado, leva em consideração a análise morfológica das palavras. Para fazer isso, é importante obter os dicionários para o algoritmo consultar a fim de vincular o formulário de volta ao seu lema. **spaCy** fornece lematização por meio de pesquisa de dicionário e cada idioma tem seu próprio dicionário.\n",
    "\n",
    "> **Dica**\n",
    "> Tanto o *Stemming* quanto a *lematização* têm suas próprias vantagens. O *Stemming* fornece resultados muito bons se você aplicar apenas algoritmos estatísticos ao texto, sem processamento semântico adicional, como pesquisa de padrão, extração de entidade, resolução de correferência e assim por diante. Também o *stemming* pode cortar um *corpus* grande para um tamanho mais moderado e fornecer uma representação compacta. Se você também usa recursos linguísticos em seu pipeline ou faz uma pesquisa por palavra-chave, inclua a *lematização*. Os algoritmos de lematização são precisos, mas têm um custo em termos de computação."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objetos de contêiner spaCy\n",
    "\n",
    "No início deste capítulo, vimos uma lista de objetos contêiner, incluindo **Doc**, **Token**, **Span** e **Lexeme**. Já usamos *Token* e *Doc* em nosso código. Nesta subseção, veremos em detalhes as propriedades dos objetos *container*.\n",
    "\n",
    "Usando objetos *container*, podemos acessar as propriedades linguísticas que *spaCy* atribui ao texto. Um objeto *contêiner* é uma representação lógica das unidades de texto, como um documento, um token ou uma fatia do documento. Objetos de *contêiner* em *spaCy* seguem a estrutura natural do texto: um documento é composto de sentenças e sentenças são compostas de tokens. Usamos mais amplamente os objetos *Doc*, *Token* e *Span* em desenvolvimento, que representam um documento, um único *token* e uma *frase*, respectivamente. Um contêiner pode conter outros contêineres, por exemplo, um documento contém *tokens* e *spans*.\n",
    "\n",
    "Vamos explorar cada classe e suas propriedades úteis uma por uma.\n",
    "\n",
    "### Doc\n",
    "\n",
    "Criamos objetos *Doc* em nosso código para representar o texto, então você já deve ter percebido que Doc representa um texto. Já sabemos como criar um objeto Doc:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "doc = nlp('I like cats')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**doc.text** retorna uma representação *Unicode* do texto do documento:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'I like cats'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O bloco de construção de um objeto *Doc* é o *Token*, portanto, quando você itera um *Doc*, obtém objetos *Token* como itens:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "like\n",
      "cats\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "\tprint(token.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "like"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mesma lógica para idexação\n",
    "\n",
    "doc[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O tamanho do <Doc> em número de tokens inclusos\n",
    "len(doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Já vimos como obter as frases do texto. **doc.sents** retorna um iterador para a lista de sentenças. Cada sentença é um objeto *Span*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator at 0x1b9699af7e0>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence. This is the second sentence\")\n",
    "doc.sents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[This is a sentence., This is the second sentence]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**doc.ents** fornece entidades nomeadas do texto. O resultado é uma lista de objetos *Span*. Veremos entidades nomeadas em detalhes mais tarde – por enquanto, pense nelas como nomes próprios:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(New York, Ashley)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I flied to New York with Ashley.\")\n",
    "doc.ents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outra propriedade sintática é **doc.noun_chunks**. Ele produz os sintagmas nominais encontrados no texto:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[Sweet brown fox, the fence]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Sweet brown fox jumped over the fence.\")\n",
    "list(doc.noun_chunks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**doc.lang_** retorna o idioma que o *doc* criou:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'en'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um método útil para serialização é **doc.to_json**. Veja como converter um objeto *Doc* em **JSON**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'Hi',\n 'ents': [],\n 'sents': [{'start': 0, 'end': 2}],\n 'tokens': [{'id': 0,\n   'start': 0,\n   'end': 2,\n   'tag': 'UH',\n   'pos': 'INTJ',\n   'morph': '',\n   'lemma': 'hi',\n   'dep': 'ROOT',\n   'head': 0}]}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Hi')\n",
    "doc.to_json()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Dica profissional**\n",
    "> Você deve ter notado que chamamos **doc.lang_**, não **doc.lang**. **doc.lang** retorna o ID do idioma, enquanto **doc.lang_** retorna a string *Unicode* do idioma, ou seja, o nome do idioma. Você pode ver a mesma convenção com recursos de token a seguir, por exemplo, **token.lemma_**, **token.tag_** e **token.pos_**.\n",
    ">\n",
    "> O objeto *Doc* tem propriedades muito úteis com as quais você pode entender as propriedades sintáticas de uma frase e usá-las em seus próprios aplicativos. Vamos passar para o objeto Token e ver o que ele oferece."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Token\n",
    "Um objeto *Token* representa uma palavra. Objetos *token* são os blocos de construção dos objetos *Doc* e *Span*. Nesta seção, abordaremos as seguintes propriedades da classe *Token*:\n",
    "* token.text\n",
    "* token.text_with_ws\n",
    "* token.i\n",
    "* token.idx\n",
    "* token.doc\n",
    "* token.enviado\n",
    "* token.is_sent_start\n",
    "* token.ent_type\n",
    "\n",
    "Normalmente, não construímos um objeto *Token* diretamente, em vez disso, construímos um objeto *Doc* e acessamos seus tokens:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Hello"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Hello Madam!')\n",
    "doc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.text** é semelhante a **doc.text** e fornece a string Unicode subjacente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.text_with_ws** é uma propriedade semelhante. Ele fornece ao texto um espaço em branco à direita, se estiver presente no documento:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello '"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].text_with_ws"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'!'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].text_with_ws"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encontrar o comprimento de um *token* é semelhante a encontrar o comprimento de uma string Python:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.i** fornece o índice do *token* em *doc*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[2]\n",
    "token.i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.idx** fornece o deslocamento de caractere do token (a posição do caractere) em **doc**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Também podemos acessar o documento que criou o token da seguinte forma:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "Hello Madam!"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[0]\n",
    "token.doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A obtenção da sentença à qual o **token** pertence é feita de maneira semelhante ao acesso ao **doc** que criou o **token**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Hello Madam!"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[0]\n",
    "token.sent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.is_sent_start** é outra propriedade útil; ele retorna um booleano indicando se o *token* inicia uma sentença:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"He entered the room. Then he nodded.\")\n",
    "doc[0].is_sent_start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[5].is_sent_start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].is_sent_start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Essas são as propriedades básicas do objeto Token que você usará todos os dias. Há outro conjunto de propriedades que estão mais relacionadas à sintaxe e semântica. Já vimos como calcular o lemma do token na seção anterior:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'go'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I went there.\")\n",
    "doc[1].lemma_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Você já aprendeu que **doc.ents** fornece as entidades nomeadas do documento. Se você quiser saber que tipo de entidade é o **token**, use **token.ent_type_**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(Mexico City,)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"President Trump visited Mexico City.\")\n",
    "doc.ents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "'GPE'"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].ent_type_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "'GPE'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[4].ent_type_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dois recursos sintáticos relacionados à marcação POS são **token.pos_** e **token.tag**. Aprenderemos o que são e como usá-los no próximo capítulo.\n",
    "\n",
    "Outro conjunto de recursos sintáticos vem do analisador de dependência. Esses recursos são **dep_**, **head_**, **conj_**, **lefts_**, **right_**, **left_edge_** e **right_edge_**. Nós vamos cobri-los no próximo capítulo também.\n",
    "\n",
    "> **Dica**\n",
    "> É totalmente normal se você não se lembrar de todos os recursos depois. Se você não se lembra do nome de um recurso, você sempre pode fazer **dir(token)** ou **dir(doc)**. Chamar **dir()** imprimirá todos os recursos e métodos disponíveis no objeto.\n",
    "> O objeto Token tem um rico conjunto de recursos, permitindo-nos processar o texto da cabeça aos pés. Vamos passar para o objeto **Span** e ver o que ele oferece para nós."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Span\n",
    "Os objetos Span representam frases ou segmentos do texto. Tecnicamente, um Span deve ser uma sequência contígua de tokens. Normalmente não inicializamos objetos *Span*, mas fatiamos um objeto **Doc**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "that you"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I know that you have been to USA.')\n",
    "doc[2:4]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tentar fatiar um índice inválido gerará um **IndexError**. A maioria das regras de indexação e fatiamento de *strings* Python também são aplicáveis ao fatiamento de documentos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "City."
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"President Trump visited Mexico City.\")\n",
    "doc[4:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Mexico City"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3:-1] # Sinal negativo é suportado"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6:] # volta vazio. Pq está fora do <range>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:1] # Vazio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Há mais uma maneira de criar um Span - podemos fazer uma fatia de nível de caractere de um objeto Doc com **char_span**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "love Atlanta"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You love Atlanta since you're 20.\")\n",
    "doc.char_span(4,16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Os blocos de construção de um objeto **Span** são objetos *Token*. Se você iterar sobre um objeto Span, obterá objetos Token:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "after\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"You went there after you saw me.\")\n",
    "span = doc[2:4]\n",
    "\n",
    "for token in span:\n",
    "\tprint(token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Você pode pensar no objeto Span como um objeto *Doc júnior*, na verdade, é uma visão do objeto Doc a partir do qual foi criado. Portanto, a maioria dos recursos do Doc também são aplicáveis ao Span. Por exemplo, **len** é idêntico:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hello Madam!\")\n",
    "span = doc[1:2]\n",
    "len(span)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O objeto Span também oferece suporte à indexação. O resultado de fatiar um objeto Span é outro objeto Span:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "there after you saw"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You went there after you saw me\")\n",
    "span = doc[2:6]\n",
    "span"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "after you"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspan = span[1:3]\n",
    "subspan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assim como um Token conhece o objeto Doc do qual foi criado; O Span também conhece o objeto Doc a partir do qual foi criado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "You went there after you saw me"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You went there after you saw me\")\n",
    "span = doc[2:6]\n",
    "span.doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Também podemos localizar o **Span** no **Doc** original:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You went there after you saw me\")\n",
    "span = doc[2:6]\n",
    "span.start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.start_char"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.end_char"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**span.start** é o índice do primeiro token do Span e **span.start_char** é o deslocamento inicial do Span no nível do caractere.\n",
    "\n",
    "Se você deseja um objeto Doc totalmente novo, pode chamar **span.as_doc()**. Ele copia os dados em um novo objeto Doc:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.tokens.span.Span"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You went there after you saw me\")\n",
    "span = doc[2:6]\n",
    "type(span)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.tokens.doc.Doc"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_doc = span.as_doc()\n",
    "type(small_doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**span.ents**, **span.sent**, **span.text** e **span.text_wth_ws** são semelhantes aos seus métodos Doc e Token correspondentes.\n",
    "\n",
    "\n",
    "### Mais recursos spacy\n",
    "\n",
    "A maior parte do desenvolvimento de NLP é orientada por token e span; ou seja, ele processa tags, relações de dependência, tokens em si e frases. Na maioria das vezes eliminamos pequenas palavras e palavras sem muito significado; processamos URLs de maneira diferente e assim por diante. O que fazemos às vezes depende da **forma do token** (token é uma palavra curta ou token se parece com uma string de URL) ou mais recursos semânticos (como o token é um artigo ou o token é uma conjunção). Nesta seção, veremos esses recursos de tokens com exemplos. Começaremos com recursos relacionados à forma do token:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "'hello'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hello, hi!\")\n",
    "doc[0].lower_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.lower_** retorna o token em letras minúsculas. O valor de retorno é uma string Unicode e esse recurso é equivalente a **token.text.lower()**. **is_lower** e **is_upper** são semelhantes aos métodos de string do Python, **islower()** e **isupper()**. **is_lower** retorna **True** se todos os caracteres são minúsculos, enquanto **is_upper** faz o mesmo com letras maiúsculas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"HELLO, Hello, hello, hEllO\")\n",
    "doc[0].is_upper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_lower"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_upper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_lower"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_alpha** retornará **True** se todos os caracteres do token forem letras alfabéticas. Exemplos de caracteres não alfabéticos são números, pontuação e espaços em branco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Cat and Cat123\")\n",
    "doc[0].is_alpha"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_alpha"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_ascii** retorna **True** se todos os caracteres do token forem caracteres ASCII."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hamburg and Göttingen\")\n",
    "doc[0].is_ascii"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_ascii"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_digit** retorna **True** se todos os caracteres do token forem números:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Cat Cat123 123\")\n",
    "doc[0].is_digit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_digit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_digit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_punct** retornará **True** se o token for um sinal de pontuação:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": ","
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"You, him and Sally\")\n",
    "doc[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_punct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_left_punct** e **is_right_punct** retornam **True** se o token for um sinal de pontuação esquerdo ou direito, respectivamente. Um sinal de pontuação à direita pode ser qualquer marca que feche um sinal de pontuação à esquerda, como colchetes à direita, `>` ou `»`. Os sinais de pontuação à esquerda são semelhantes, com os colchetes esquerdos `<` e `«` como alguns exemplos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "("
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"( [ He said yes. ] )\")\n",
    "doc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_left_punct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": ")"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-1].is_right_punct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "]"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-2].is_right_punct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_space** retorna **True** se o token for apenas caracteres de espaço em branco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": " "
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\" \")\n",
    "doc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "  "
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"  \")\n",
    "doc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_bracket** retorna **True** para caracteres de colchetes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, True)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"( You said [1] and {2} is not applicable.)\")\n",
    "doc[0].is_bracket, doc[-1].is_bracket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, True)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_bracket, doc[5].is_bracket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, True)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[7].is_bracket, doc[9].is_bracket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_quote** retorna **True** para aspas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "'"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"( You said '1\\\" is not applicable.)\")\n",
    "doc[3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_quote"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "\""
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[5].is_quote"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_currency** retorna **True** para símbolos de moeda como **$** e **€**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "$"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I paid 12$ for the tshirt.\")\n",
    "doc[3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_currency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**like_url**, **like_num** e **like_email** são métodos sobre a forma do token e retornam **True** se o token se parecer com uma **URL**, um **número** ou um **email**, respectivamente. Esses métodos são muito úteis quando queremos processar texto de mídia social e páginas da web raspadas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I emailed you at least 100 times\")\n",
    "doc[-2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-2].like_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "hundred"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I emailed you at least hundred times\")\n",
    "doc[-2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-2].like_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "email@hotmail.com"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Meu email é email@hotmail.com e você pode me visitar em https://site.com.br quando quiser.\")\n",
    "doc[3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].like_email"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "https://site.com.br"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[10].like_url"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**token.shape_** é um recurso incomum – não há nada semelhante em outras bibliotecas de NLP. Ele retorna uma string que mostra os recursos ortográficos de um token. Os números são substituídos por **d**, as letras maiúsculas são substituídas por X e as letras minúsculas são substituídas por x. Você pode usar a string de resultado como um recurso em seus algoritmos de aprendizado de máquina e as formas de token podem ser correlacionadas ao sentimento do texto:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN TEXT: Girl =============> TOKEN SHAPE: Xxxx\n",
      "TOKEN TEXT: called =============> TOKEN SHAPE: xxxx\n",
      "TOKEN TEXT: Kathy =============> TOKEN SHAPE: Xxxxx\n",
      "TOKEN TEXT: has =============> TOKEN SHAPE: xxx\n",
      "TOKEN TEXT: a =============> TOKEN SHAPE: x\n",
      "TOKEN TEXT: nickname =============> TOKEN SHAPE: xxxx\n",
      "TOKEN TEXT: Cat123 =============> TOKEN SHAPE: Xxxddd\n",
      "TOKEN TEXT: . =============> TOKEN SHAPE: .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\n",
    "for token in doc:\n",
    "    print(f\"TOKEN TEXT: {token.text} =============> TOKEN SHAPE: {token.shape_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_oov** e **is_stop** são recursos semânticos, em oposição aos recursos de forma anteriores. **is_oov** retornará **True** se o token for **Out Of Vocabulary (OOV)**, ou seja, não estiver no vocabulário do objeto **Doc**. As palavras **OOV** são palavras desconhecidas para o modelo de linguagem e, portanto, também para os componentes do pipeline de processamento:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ======== False\n",
      "visited ======== False\n",
      "Jenny ======== False\n",
      "at ======== False\n",
      "Mynks ======== True\n",
      "Resort ======== False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I visited Jenny at Mynks Resort\")\n",
    "for token in doc:\n",
    "\tprint(f\"{token} ======== {token.is_oov}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**is_stop** é um recurso frequentemente usado por algoritmos de aprendizado de máquina. Muitas vezes, filtramos palavras que não carregam muito significado, como *the*, *a*, *an*, *and*, *just*, *with*, e assim por diante. Essas palavras são chamadas de palavras de parada. Cada idioma tem sua própria lista de palavras de parada, e você pode acessar as palavras de parada em inglês aqui https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ======== True\n",
      "just ======== True\n",
      "want ======== False\n",
      "to ======== True\n",
      "inform ======== False\n",
      "you ======== True\n",
      "that ======== True\n",
      "I ======== True\n",
      "was ======== True\n",
      "with ======== True\n",
      "the ======== True\n",
      "principle ======== False\n",
      ". ======== False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just want to inform you that I was with the principle.\")\n",
    "for token in doc:\n",
    "    print(f\"{token} ======== {token.is_stop}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esgotamos a lista de recursos sintáticos, semânticos e ortográficos do spaCy. Sem surpresa, muitos métodos focados no objeto Token como um token são a unidade sintática de um texto."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
