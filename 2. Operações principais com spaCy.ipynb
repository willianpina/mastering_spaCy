{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=center>Capítulo 2 - Operações principais com spaCy</h1>\n",
    "<p align=center><img src=https://www.edivaldobrito.com.br/wp-content/uploads/2021/02/spacy-uma-biblioteca-de-processamento-de-linguagem-natural.jpg width=500></p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neste capítulo, você aprenderá as principais operações com **spaCy**, como criar um pipeline de linguagem, tokenizar o texto e dividir o texto em frases.\n",
    "\n",
    "Primeiro, você aprenderá o que é um pipeline de processamento de linguagem e os componentes do pipeline. Continuaremos com as convenções gerais de spaCy – aulas importantes e organização de classes – para ajudá-lo a entender melhor a organização da biblioteca spaCy e desenvolver uma compreensão sólida da própria biblioteca.\n",
    "\n",
    "Você aprenderá então sobre o primeiro componente do pipeline – **Tokenizer**. Você também aprenderá sobre um importante conceito linguístico – **lematização** – juntamente com suas aplicações na **compreensão da linguagem natural (NLU)**.\n",
    "\n",
    "Em seguida, abordaremos as **classes de contêiner** e as **estruturas de dados spaCy** em detalhes. Terminaremos o capítulo com recursos úteis que você usará no desenvolvimento diário de PNL.\n",
    "\n",
    "## Visão geral das convenções spaCy\n",
    "Cada aplicação de PNL consiste em várias etapas de processamento do texto. Como você pode ver no primeiro capítulo, sempre criamos instâncias chamadas **nlp** e **doc*. Mas o que fizemos exatamente?\n",
    "\n",
    "Quando chamamos **nlp** em nosso texto, **spaCy** aplica algumas etapas de processamento. A primeira etapa é a tokenização para produzir um objeto **Doc**. O objeto **Doc** é então processado com um **tagger**, um **analisador (parser)** e um **reconhecedor de entidade (entity recognizer)**. Essa maneira de processar o texto é chamada de **pipeline de processamento de linguagem**. Cada componente da pipeline retorna o **Doc** processado e o passa para o próximo componente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src='images/pipeline_tokenizer.PNG' width=900>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um objeto de **pipeline spaCy** é criado quando carregamos um modelo de linguagem. Carregamos um modelo em inglês e inicializamos um pipeline no seguinte segmento de código:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('I went there')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O que aconteceu exatamente no código anterior é o seguinte:\n",
    "1. Começamos importando **spaCy**.\n",
    "2. Na segunda linha, **spacy.load()** retornou uma instância da classe **Language**, **nlp**. A classe **Language** é o *pipeline de processamento de texto*.\n",
    "3. Depois disso, aplicamos **nlp** na frase de exemplo **\"I went there\"** e peguei uma instância da classe **Doc**, **doc**.\n",
    "\n",
    "A classe **Language** aplica todas as etapas anteriores do pipeline à sua frase de entrada nos bastidores. Depois de aplicar **nlp** à sentença, o objeto **Doc** contém tokens que são marcados, lematizados e marcados como entidades se o token for uma entidade (então entraremos em detalhes sobre o que são e como isso é feito posteriormente). Cada componente do pipeline tem uma tarefa bem definida:\n",
    "\n",
    "<img src=\"images/pipeline_components.PNG\" width=900>\n",
    "\n",
    "O pipeline de processamento de linguagem **spaCy** sempre depende do modelo estatístico e de seus recursos. É por isso que sempre carregamos um modelo de linguagem com **spacy.load()** como o primeiro passo em nosso código.\n",
    "\n",
    "Cada componente corresponde a uma classe **spaCy**. As classes **spaCy** têm nomes autoexplicativos, como **Language**, **Doc** e **Vocab**. Já usamos as classes **Language** e **Doc** – vamos ver todas as classes do pipeline de processamento e suas funções:\n",
    "\n",
    "<img src=\"images/processing_pipeline.png\">\n",
    "\n",
    "Não fique intimidade com o número de classes. Cada classe tem um característica única para nos ajudar a processar o texto melhor.\n",
    "\n",
    "Existem mais estruturas de dados para representar os dados de texto e os dados da linguagem. A classe Conteiner como a Doc armazena as informações sobre as sentenças, palavras e o texto. Existem outras classes conteiner além da Doc:\n",
    "\n",
    "<img src=\"images/conteiner_classes.PNG\" width=900>\n",
    "\n",
    "Finalmente, spaCy fornece classes auxiliares para vetores, vocabulário de linguagem e anotações. Veremos a classe **Vocab** frequentemente neste livro. **Vocab** representa o vocabulário de uma língua. Vocab contém todas as palavras do modelo de linguagem que carregamos:\n",
    "\n",
    "<img src=\"images/outras_classes.PNG\" width=900>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As estruturas de dados do backbone da biblioteca spaCy são **Doc** e **Vocab**. O objeto **Doc** abstrai o texto possuindo a sequência de tokens e todas as suas propriedades. O objeto **Vocab** fornece um conjunto centralizado de **strings** e atributos léxicos para todas as outras classes. Dessa forma, o **spaCy** evita o armazenamento de várias cópias de dados linguísticos:\n",
    "\n",
    "<img src=\"images/diagrama_spacy.PNG\">\n",
    "\n",
    "Você pode dividir os objetos que compõem a arquitetura spaCy anterior em dois: **contêineres** e **componentes de pipeline de processamento**. Neste capítulo, primeiro aprenderemos sobre dois componentes básicos, **Tokenizer** e **Lemmatizer**, depois exploraremos mais os objetos **Container**.\n",
    "\n",
    "**spaCy** faz todas essas operações para nós nos bastidores, permitindo que nos concentremos no desenvolvimento de nosso próprio aplicativo. Com esse nível de abstração, usar **spaCy** para desenvolvimento de aplicativos NLP não é coincidência. Vamos começar com a classe **Tokenizer** e ver o que ela oferece para nós; em seguida, exploraremos todas as classes de contêineres uma a uma ao longo do capítulo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apresentando a tokenização\n",
    "Vimos que a primeira etapa em um pipeline de processamento de texto é a tokenização. A tokenização é sempre a primeira operação porque todas as outras operações requerem os tokens.\n",
    "\n",
    "Tokenização significa simplesmente dividir a sentença em seus tokens. Um token é uma unidade de semântica. Você pode pensar em um token como a menor parte significativa de um pedaço de texto. Os tokens podem ser palavras, números, pontuação, símbolos de moeda e quaisquer outros símbolos significativos que são os blocos de construção de uma frase. Seguem exemplos de tokens:\n",
    "* USA\n",
    "* N.Y\n",
    "* 33\n",
    "* 3rd\n",
    "* !\n",
    "* ...\n",
    "* ?\n",
    "* 's\n",
    "\n",
    "A entrada para o tokenizer **spaCy** é um texto Unicode e o resultado é um objeto **Doc**. O código a seguir mostra o processo de tokenização:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'm', 'own', 'Giger', 'cat', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Im own Giger cat.\")\n",
    "print([token.text for token in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O seguinte é o que acabamos de fazer:\n",
    "1. Começamos importando **spaCy**.\n",
    "2. Em seguida, carregamos o modelo de idioma inglês por meio do atalho **en** para criar uma instância da **classe nlp Language**.\n",
    "3. Em seguida, aplicamos o objeto **nlp** à sentença de entrada para criar um objeto **Doc**, **doc**. Um objeto **Doc** é um contêiner para uma sequência de objetos **Token**. **spaCy** gera os objetos **Token** implicitamente quando criamos o objeto **Doc**.\n",
    "4. Finalmente, imprimimos uma lista dos tokens da sentença anterior.\n",
    "\n",
    "É isso, fizemos a tokenização com apenas três linhas de código. Você pode visualizar a tokenização com indexação da seguinte forma:\n",
    "\n",
    "<img src=\"images/tokenização_im_own_giver_cat.PNG\" width=500>\n",
    "\n",
    "Como os exemplos sugerem, a tokenização pode realmente ser complicada. Há muitos aspectos aos quais devemos prestar atenção: pontuação, espaços em branco, números e assim por diante. Dividir os espaços em branco com **text.split(\" \")** pode ser tentador e parece que está funcionando para a frase de exemplo que *I own a ginger cat*.\n",
    "\n",
    "Que tal a frase **\"It´s been a crazy week!!!\"**? Se fizermos um **split(\" \")** os tokens resultantes seriam **It's, been, a, crazy, week!!!**, que não é o que você deseja. Em primeiro lugar, **It's** não é um token, são dois tokens: **it** e **'s**. **week!!!** não é um token válido, pois a pontuação não está dividida corretamente. Além disso, **!!!** deve ser tokenizado por símbolo e deve gerar três **!**. Isso pode não parecer um detalhe importante, mas acredite, é importante para a análise de sentimentos. Vamos ver o que o tokenizer **spaCy** gerou:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"It's been a crazy week!!!\")\n",
    "print([token.text for token in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como o spaCy sabe onde dividir a frase? Ao contrário de outras partes do pipeline, o tokenizer não precisa de um modelo estatístico. A tokenização é baseada em regras específicas do idioma.\n",
    "\n",
    "As exceções do **tokenizer** definem regras para exceções, como **it's**, **don't**, **won't**, abreviaturas e assim por diante. você verá que as regras se parecem com {ORTH: \"n't\", LEMMA:\"not\"}, que descreve a regra de divisão de n't para o tokenizer.\n",
    "\n",
    "Os prefixos, sufixos e infixos descrevem principalmente como lidar com pontuação – por exemplo, dividimos em um ponto se estiver no final da frase, caso contrário, provavelmente é parte de uma abreviação como N.Y. e não deveríamos toque isso. Aqui, **ORTH** significa o texto e **LEMMA** significa a forma da palavra base sem quaisquer inflexões. O exemplo a seguir mostra a execução do algoritmo de tokenização spaCy:\n",
    "\n",
    "<img src=\"images/rules_tokenization.PNG\" width=500>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As regras de tokenização dependem das regras gramaticais do idioma individual. As regras de pontuação, como pontos de divisão, vírgulas ou pontos de exclamação, são mais ou menos semelhantes para muitos idiomas; no entanto, algumas regras são específicas para o idioma individual, como palavras de abreviação e uso de apóstrofos. **spaCy** suporta cada idioma com suas próprias regras específicas, permitindo dados e regras codificados à mão, pois cada idioma tem sua própria subclasse.\n",
    "\n",
    "> **Dica**\n",
    "\n",
    "> **spaCy** fornece tokenização *não destrutiva*, o que significa que sempre poderemos recuperar o texto original dos tokens. As informações de espaço em branco e pontuação são preservadas durante a tokenização, portanto, o texto de entrada é preservado como está.\n",
    "> Cada objeto **Language** contém um objeto **Tokenizer**. A classe **Tokenizer** é a classe que executa a tokenização. Você não costuma chamar essa classe diretamente quando cria uma instância de classe **Doc**, enquanto a classe **Tokenizer** atua nos bastidores. Quando queremos customizar a tokenização, precisamos interagir com essa classe. Vamos ver como é feito.\n",
    "\n",
    "## Personalizando o tokenizer\n",
    "Quando trabalhamos com um domínio específico, como medicina, seguros ou finanças, muitas vezes nos deparamos com palavras, abreviações e entidades que precisam de atenção especial. A maioria dos domínios que você processará tem palavras e frases características que precisam de regras de tokenização personalizadas. Veja como adicionar uma regra de caso especial a uma instância de classe **Tokenizer** existente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemme', 'that']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('lemme that')\n",
    "print([w.text for w in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "special_case = [{ORTH:'lem'},{ORTH:'me'}]\n",
    "nlp.tokenizer.add_special_case('lemme',special_case)\n",
    "print([w.text for w in nlp('lemme that')])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui está o que nós fizemos:\n",
    "1. Começamos novamente importando o **spaCy**.\n",
    "2. Em seguida, importamos o símbolo **ORTH**, que significa ortografia; isto é, texto.\n",
    "3. Continuamos com a criação de um objeto de classe **Language**, **nlp**, e criamos um objeto *Doc*, **doc**.\n",
    "4. Definimos um caso especial, onde a palavra **lemme** deve ser tokenizada como dois tokens, **lem** e **me**.\n",
    "5. Adicionamos a regra ao tokenizer do objeto **nlp**.\n",
    "6. A última linha mostra como a nova regra funciona.\n",
    "\n",
    "Quando definimos regras personalizadas, as regras de divisão de pontuação ainda serão aplicadas. Nosso caso especial será reconhecido como resultado, mesmo que esteja cercado por pontuação. O tokenizer dividirá a pontuação passo a passo e aplicará o mesmo processo à substring restante:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', '!']\n"
     ]
    }
   ],
   "source": [
    "print([w.text for w in nlp('lemme!')])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se você definir uma regra de caso especial com pontuação, a regra de caso especial terá precedência sobre a divisão de pontuação:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...lemme...?']\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"...lemme...?\", [{\"ORTH\": \"...lemme...?\"}])\n",
    "print([w.text for w in nlp(\"...lemme...?\")])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Dica profissional**\n",
    "Modifique o tokenizer adicionando novas regras *somente se você realmente precisar*. Confie em mim, você pode obter resultados bastante inesperados com regras personalizadas. Um dos casos em que você realmente precisa é ao trabalhar com o texto do *Twitter*, que geralmente está cheio de *hashtags* e símbolos especiais. Se você tiver texto de mídia social, primeiro insira algumas frases no *pipeline spaCy NLP* e veja como a tokenização funciona.\n",
    "\n",
    "## Depurando o tokenizer\n",
    "A biblioteca spaCy possui uma ferramenta para depuração:\n",
    "\n",
    "**nlp.tokenizer.explain(sentence)**. Ele retorna tuplas **(tokenizer rule/pattern,token)** para nos ajudar a entender o que aconteceu exatamente durante a tokenização. Vejamos um exemplo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let  --------------> SPECIAL-1\n",
      "'s  --------------> SPECIAL-2\n",
      "go  --------------> TOKEN\n",
      "!  --------------> SUFFIX\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Let's go!\"\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "for t in tok_exp:\n",
    "    print(f\"{t[1]}  --------------> {t[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No código anterior, importamos o **spaCy** e criamos uma instância da classe **Language**, **nlp**, como de costume. Em seguida, criamos uma instância da classe *Doc* com a frase **Let's go!**. Em seguida, solicitamos à instância da classe *Tokenizer*, **tokenizer**, do *nlp* uma explicação sobre a tokenização desta sentença. **nlp.tokenizer.explain()** explicou as regras que o tokenizer usou uma a uma.\n",
    "\n",
    "Depois de dividir uma frase em seus tokens, é hora de dividir um texto em suas frases.\n",
    "\n",
    "## Segmentação de frases\n",
    "Vimos que quebrar uma sentença em seus tokens não é uma tarefa simples. Que tal quebrar um texto em frases? É realmente um pouco mais complicado marcar onde uma frase começa e termina devido aos mesmos motivos de pontuação, abreviações e assim por diante.\n",
    "\n",
    "As sentenças de um objeto *Doc* estão disponíveis através da propriedade **doc.sents**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I flied to N.Y yesterday.\n",
      "It was around 5 pm.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determinar os limites da frase é uma tarefa mais complicada do que a tokenização. Como resultado, spaCy usa o analisador de dependência para realizar a segmentação de sentenças. Esta é uma característica única do spaCy – nenhuma outra biblioteca põe em prática uma ideia tão sofisticada. Os resultados são muito precisos em geral, a menos que você processe texto de um gênero muito específico, como do domínio da conversa ou texto de mídia social.\n",
    "\n",
    "Agora sabemos como segmentar um texto em frases e tokenizar as frases. Estamos prontos para processar os tokens um por um. Vamos começar com a *lematização*, uma operação comumente usada em semântica, incluindo análise de sentimentos.\n",
    "\n",
    "**Entendendo a lematização**\n",
    "\n",
    "Um **lemma** é a forma base de um token. Você pode pensar em um *lemma* como a forma na qual o token aparece em um dicionário. Por exemplo, o *lemma* de *eating* é *eat*; o *lemma* de *eats* é *eat*; *ate* da mesma forma mapeia para *eat*. *Lematização* é o processo de reduzir as formas das palavras aos seus *lemmas*. O código a seguir é um exemplo rápido de como fazer lematização com spaCy:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: I == LEMMA: I\n",
      "TOKEN: went == LEMMA: go\n",
      "TOKEN: there == LEMMA: there\n",
      "TOKEN: for == LEMMA: for\n",
      "TOKEN: working == LEMMA: work\n",
      "TOKEN: and == LEMMA: and\n",
      "TOKEN: worked == LEMMA: work\n",
      "TOKEN: for == LEMMA: for\n",
      "TOKEN: 3 == LEMMA: 3\n",
      "TOKEN: years == LEMMA: year\n",
      "TOKEN: . == LEMMA: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I went there for working and worked for 3 years.\")\n",
    "for token in doc:\n",
    "    print(f\"TOKEN: {token.text} == LEMMA: {token.lemma_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Até agora, você deve estar familiarizado com o que as três primeiras linhas do código fazem. Lembre-se de que importamos a biblioteca **spacy**, carregamos um modelo em inglês usando **spacy.load**, criamos um pipeline e aplicamos o pipeline à frase anterior para obter um objeto **Doc**. Aqui nós iteramos sobre tokens para obter seu *texto* e *lemmas*.\n",
    "\n",
    "Este é um lemma pronome, um símbolo especial para lemas de pronomes pessoais. Esta é uma exceção para fins semânticos: os pronomes pessoais *você*, *eu*, *tu*, *ele*, *dele* e assim por diante, parecem diferentes, mas em termos de significado, eles estão no mesmo grupo. *spaCy* oferece este truque para os lemas do pronome.\n",
    "\n",
    "Não se preocupe se tudo isso soar muito abstrato – vamos ver a lematização em ação com um exemplo do mundo real.\n",
    "\n",
    "### Lematização em NLU\n",
    "\n",
    "A lematização é um passo importante na NLU. Veremos um exemplo nesta subseção. Suponha que você crie um pipeline de NLP para um sistema de reserva de passagens. Seu aplicativo processa a sentença de um cliente, extrai dela as informações necessárias e a passa para a API de reservas.\n",
    "\n",
    "O pipeline de NLP deseja extrair a forma da viagem (*a flight*, *bus*, ou *train*), a *cidade de destino* e a *data*. A primeira coisa que o aplicativo precisa verificar é o meio de viagem:\n",
    "\n",
    "* *fly* – *flight* – *airway* – *airplane* - *plane*\n",
    "\n",
    "* *bus*\n",
    "\n",
    "* *railway* – *train*\n",
    "\n",
    "Temos essa lista de palavras-chave e queremos reconhecer os meios de viagem pesquisando os tokens na lista de palavras-chave. A maneira mais compacta de fazer essa pesquisa é pesquisar o lemma do token. Considere as seguintes frases de clientes:\n",
    "\n",
    "* List me all flights to Atlanta.\n",
    "* I need a flight to NY.\n",
    "* I flew to Atlanta yesterday evening and forgot my baggage.\n",
    "\n",
    "Aqui, não precisamos incluir todas as formas de palavras do verbo *fly* (*fly, flying, flies, flew, and flown*) na lista de palavras-chave e similares para a palavra **flight**; reduzimos todas as variantes possíveis para as formas básicas – *fly* e *flight*. Não pense apenas em inglês; línguas como espanhol, alemão e finlandês também têm muitas formas de palavras de um único lemma.\n",
    "\n",
    "A *lematização* também é útil quando queremos reconhecer a cidade de destino. Existem muitos apelidos disponíveis para cidades globais e a API de reservas pode processar apenas os nomes oficiais. O tokenizer e o lematizer padrão não saberão a diferença entre o nome oficial e o apelido. Nesse caso, você pode adicionar regras especiais, como vimos na seção *Introdução à tokenização*. O código a seguir desempenha um pequeno truque:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: I == LEMMA: i\n",
      "TOKEN: am == LEMMA: am\n",
      "TOKEN: flying == LEMMA: flying\n",
      "TOKEN: to == LEMMA: to\n",
      "TOKEN: Angeltown == LEMMA: Los Angeles\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH, NORM, LEMMA\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "special_case = [{ORTH: 'Angeltown', NORM: 'Los Angeles'}]\n",
    "nlp.tokenizer.add_special_case('Angeltown', special_case)\n",
    "doc = nlp(u'I am flying to Angeltown')\n",
    "for token in doc:\n",
    "    print(f\"TOKEN: {token.text} == LEMMA: {token.norm_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos um caso especial para a palavra Angeltown substituindo seu lema pelo nome oficial Los Angeles. Em seguida, adicionamos esse caso especial à instância do Tokenizer. Quando imprimimos os lemas token, vemos que Angeltown mapeia para Los Angeles como desejávamos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um *lemma* é a forma base de uma palavra e é sempre um membro do vocabulário da língua. O radical não precisa ser uma palavra válida. Por exemplo, o lemma da *improvement* é *improvement*, mas o radical é *improv*. Você pode pensar no radical como a menor parte da palavra que carrega o significado. Compare os seguintes exemplos:\n",
    "Word       |      Lemma\n",
    "-----------|------------\n",
    "university | university\n",
    "universe|  universe\n",
    "universal | universal\n",
    "universities| university\n",
    "universes| universe\n",
    "improvement | improvement\n",
    "improvements |improvements\n",
    "improves | improve\n",
    "Os exemplos de lemas de palavras anteriores mostram como o lema é calculado seguindo as regras gramaticais do idioma. Aqui, o lema de uma forma plural é a forma singular, e o lema de um verbo de terceira pessoa é a forma básica do verbo. Vamos compará-los com os seguintes exemplos de pares de radicais de palavras:\n",
    "Raiz da palavra\n",
    "universidades universitárias\n",
    "universo universo\n",
    "universos universais\n",
    "universidades universi\n",
    "universos universos\n",
    "melhoria melhoria\n",
    "melhorias melhorar\n",
    "melhora a improvisação\n",
    "O primeiro e mais importante ponto a ser observado nos exemplos anteriores é que o lema não precisa ser uma palavra válida na linguagem. O segundo ponto é que muitas palavras podem mapear para o mesmo radical. Além disso, palavras de diferentes categorias gramaticais podem ser mapeadas para o mesmo radical; aqui, por exemplo, o substantivo melhora e o verbo melhora ambos mapeiam para improvisar.\n",
    "Embora os radicais não sejam palavras válidas, eles ainda carregam significado. É por isso que o stemming é comumente usado em aplicativos NLU. Algoritmos de derivação não sabem nada sobre a gramática do\n",
    "Língua. Essa classe de algoritmos funciona cortando alguns sufixos e prefixos comuns do início ou do final da palavra. Os algoritmos de Stemming são grosseiros, eles cortam a palavra da cabeça e da cauda. Existem vários algoritmos de stemming disponíveis para inglês, incluindo Porter e Lancaster. Você pode jogar com diferentes algoritmos de stemming na página de demonstração do NLTK em https://text-processing.com/demo/stem/. A lematização, por outro lado, leva em consideração a análise morfológica das palavras. Para fazer isso, é importante obter os dicionários para o algoritmo consultar a fim de vincular o formulário de volta ao seu lema. spaCy fornece lematização por meio de pesquisa de dicionário e cada idioma tem seu próprio dicionário.\n",
    "Dica\n",
    "Tanto a lematização quanto a lematização têm suas próprias vantagens. A derivação fornece resultados muito bons se você aplicar apenas algoritmos estatísticos ao texto, sem processamento semântico adicional, como pesquisa de padrão, extração de entidade, resolução de correferência e assim por diante. Também o stemming pode cortar um corpus grande para um tamanho mais moderado e fornecer uma representação compacta. Se você também usa recursos linguísticos em seu pipeline ou faz uma pesquisa por palavra-chave, inclua a lematização. Os algoritmos de lematização são precisos, mas têm um custo em termos de computação."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
